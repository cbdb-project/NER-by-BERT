{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.tsv\r\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM DEFAULT CODE, YOU DON'T NEED TO PAY ATTENTION\n",
    "\n",
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SYSTEM DEFAULT CODE, YOU DON'T NEED TO PAY ATTENTION\n",
    "\n",
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 12.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM DEFAULT CODE, YOU DON'T NEED TO PAY ATTENTION\n",
    "\n",
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, you need to use the persistence path as the following:\n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SYSTEM DEFAULT CODE, YOU DON'T NEED TO PAY ATTENTION\r\n",
    "\r\n",
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可:\r\n",
    "# Also add the following code, so that every time the environment (kernel) starts, just run the following code:\r\n",
    "import sys\r\n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlehub==1.8.2\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b8/8d/46b67feae675d0ac106234b3c5806ba6198719fe850d61381c3311cdea6c/paddlehub-1.8.2-py3-none-any.whl (336kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 13.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (1.21.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (1.15.0)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (4.1.0)\n",
      "Requirement already satisfied: yapf==0.26.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (0.26.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (4.36.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (0.1.85)\n",
      "Requirement already satisfied: gunicorn>=19.10.0; sys_platform != \"win32\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (20.0.4)\n",
      "Requirement already satisfied: cma>=2.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (2.7.0)\n",
      "Requirement already satisfied: flask>=1.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (1.1.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (3.14.0)\n",
      "Requirement already satisfied: flake8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (3.8.2)\n",
      "Requirement already satisfied: pandas; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (1.1.5)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (3.4.5)\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.8.2) (2.1.1)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (0.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (0.23)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (2.0.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (1.4.10)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (5.1.2)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (16.7.9)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (1.3.0)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.8.2) (1.3.4)\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gunicorn>=19.10.0; sys_platform != \"win32\"->paddlehub==1.8.2) (41.4.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.8.2) (0.16.0)\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.8.2) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.8.2) (2.10.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.8.2) (1.1.0)\n",
      "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.8.2) (2.6.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.8.2) (0.6.1)\n",
      "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.8.2) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas; python_version >= \"3\"->paddlehub==1.8.2) (1.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas; python_version >= \"3\"->paddlehub==1.8.2) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas; python_version >= \"3\"->paddlehub==1.8.2) (2019.3)\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlehub==1.8.2) (0.7.1.1)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlehub==1.8.2) (1.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlehub==1.8.2) (2.22.0)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlehub==1.8.2) (0.8.53)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlehub==1.8.2) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->paddlehub==1.8.2) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.0->paddlehub==1.8.2) (1.1.1)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlehub==1.8.2) (2.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlehub==1.8.2) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlehub==1.8.2) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlehub==1.8.2) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlehub==1.8.2) (2.8)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlehub==1.8.2) (0.18.0)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlehub==1.8.2) (3.9.9)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->paddlehub==1.8.2) (7.2.0)\n",
      "Installing collected packages: paddlehub\n",
      "  Found existing installation: paddlehub 2.0.4\n",
      "    Uninstalling paddlehub-2.0.4:\n",
      "      Successfully uninstalled paddlehub-2.0.4\n",
      "Successfully installed paddlehub-1.8.2\n"
     ]
    }
   ],
   "source": [
    "#Install paddlehub environment, must with version 1.8.2! Or there will be errors\r\n",
    "\r\n",
    "!pip install paddlehub==1.8.2\r\n",
    "import paddlehub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import json\r\n",
    "import time\r\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next block, we load the pretrained BERT model. We select a slight model for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "# Load BERT pretrained model\r\n",
    "model=hub.Module(name=\"bert_chinese_L-12_H-768_A-12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The next block will be the data preprocessing block.\n",
    "\n",
    "In the next block, we do some data preprocessing. The training data is stored in a .json file named result.json (I know this name looks very misleading). The code in the block unpack the .json file, arrange them into ndarray objects: x_data and y_data, which corresponds to the texts and the tags.\n",
    "\n",
    "Next, we convert the dataset to DataFrame objects and split them into train, validate and test set randomly.\n",
    "\n",
    "We also notice that we have to save the dataset into .tsv files in the next section. To do this, we separate the characters using unseen \\002 markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data in result.json is the labeled ming data\r\n",
    "with open('/home/aistudio/result.json') as jf:\r\n",
    "    ming=json.load(jf)\r\n",
    "\r\n",
    "\r\n",
    "#unpack .json file, wash the data and transfer them into ndarray\r\n",
    "# x_data, y_data corresponds to texts and tags\r\n",
    "def dataGen(ming):\r\n",
    "    person_ids=np.array(list(ming.keys()))\r\n",
    "    person_ids.sort()\r\n",
    "\r\n",
    "    #idNum=len(person_ids)\r\n",
    "\r\n",
    "    x_data=[]\r\n",
    "    y_data=[]\r\n",
    "\r\n",
    "    indexer=0\r\n",
    "    for person_id in person_ids:\r\n",
    "        \r\n",
    "        char_tag=ming[person_id]['char_tag']\r\n",
    "        x_data.append([])\r\n",
    "        y_data.append([])\r\n",
    "        omit_len=len(person_id)\r\n",
    "        for i in range(omit_len+1,len(char_tag)):\r\n",
    "            x_data[indexer].append(char_tag[i][0])\r\n",
    "            y_data[indexer].append(char_tag[i][1])\r\n",
    "         \r\n",
    "        indexer=indexer+1\r\n",
    "\r\n",
    "    for i in range(0,len(y_data)):\r\n",
    "      for j in range(0,len(y_data[i])):\r\n",
    "        old_text=y_data[i][j]\r\n",
    "        #convert labels like 'B_date_reign' into 'B-date-reign'\r\n",
    "        new_text=old_text.replace(\"_\",\"-\")\r\n",
    "        y_data[i][j]=new_text\r\n",
    "\r\n",
    "\r\n",
    "    return x_data,y_data,person_ids\r\n",
    "\r\n",
    "\r\n",
    "#construct train,validate and test set\r\n",
    "#train_set_rate indicates the proportion of trainning data\r\n",
    "#validate_set_rate indicates the proportion of validation data\r\n",
    "def splitTrain(x_data,y_data,person_ids,train_set_rate,validate_set_rate):\r\n",
    "    x_data=np.array(x_data)\r\n",
    "    y_data=np.array(y_data)\r\n",
    "    \r\n",
    "    temp=np.array([x_data,y_data])\r\n",
    "    temp=temp.T\r\n",
    "    \r\n",
    "    ming_data=pd.DataFrame(temp,index=person_ids,columns=['text_a','label'])\r\n",
    "\r\n",
    "    #To save data as .tsv files, we must separate the characters using \\002 marker\r\n",
    "    for i in range(0,len(ming_data['text_a'])):\r\n",
    "        #print(i)\r\n",
    "        ming_data['text_a'][i]='\\002'.join(ming_data['text_a'][i])\r\n",
    "        ming_data['label'][i]='\\002'.join(ming_data['label'][i])\r\n",
    "        ##ming_data['text_a'][i]+='\\002'\r\n",
    "        ##ming_data['label'][i]+='\\002'\r\n",
    "        #ming_data['text_a'][i]=str(ming_data['text_a'][i])\r\n",
    "        #ming_data['label'][i]=str(ming_data['label'][i])\r\n",
    "        \r\n",
    "        \r\n",
    "\r\n",
    "    np.random.seed(int(time.time()))\r\n",
    "    ming_data=ming_data.sample(frac=1.0)\r\n",
    "    \r\n",
    "    idNum=len(person_ids)\r\n",
    "    train_size=int(np.floor(idNum*train_set_rate))\r\n",
    "    validate_size=int(np.floor(idNum*validate_set_rate))\r\n",
    "\r\n",
    "    train_set=ming_data[0:train_size]\r\n",
    "\r\n",
    "    validate_set=ming_data[train_size:train_size+validate_size]\r\n",
    "    test_set=ming_data[train_size+validate_size:idNum]\r\n",
    "    return train_set,validate_set,test_set,ming_data\r\n",
    "\r\n",
    "\r\n",
    "#Build the first model: 50% of ming as train and 20% of ming as test\r\n",
    "x_data,y_data,person_ids=dataGen(ming)\r\n",
    "t,v,testing,ming_data=splitTrain(x_data,y_data,person_ids,0.75,0.25)\r\n",
    "\r\n",
    "#generate x,y of train,validate and test\r\n",
    "x_train=np.array(t['text_a'])\r\n",
    "y_train=np.array(t['label'])\r\n",
    "x_validate=np.array(v['text_a'])\r\n",
    "y_validate=np.array(v['label'])\r\n",
    "\r\n",
    "x_test=np.array(testing['text_a'])\r\n",
    "y_test=np.array(testing['label'])\r\n",
    "\r\n",
    "# We can save them as .csv files if we want\r\n",
    "t.to_csv('train.csv')\r\n",
    "v.to_csv('validate.csv')\r\n",
    "\r\n",
    "# Acquire the length of training set\r\n",
    "train_len=[]\r\n",
    "for i in range(0,len(x_train)):\r\n",
    "  train_len.append(len(x_train[i]))\r\n",
    "\r\n",
    "max_len=max(train_len)\r\n",
    "\r\n",
    "# The tag list, which specifies how many tags are of interest.\r\n",
    "tag_list = tag_list = [\"O\",\r\n",
    "        \"B_date_reign\", \"I_date_reign\",\r\n",
    "        \"B_date_year\", \"I_date_year\",\r\n",
    "        \"B_office_voa\", \"I_office_voa\",\r\n",
    "        \"B_office_title\", \"I_office_title\",\r\n",
    "        \"B_place_placename\", \"I_place_placename\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The next code block save our data to .tsv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part of Task1: save the data into tsv\r\n",
    "# t-> train, v-> validate, testing -> test\r\n",
    "# ming_data -> whole dataset, also saved in case it might be of use\r\n",
    "t.to_csv('/home/aistudio/train.tsv',sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\r\n",
    "v.to_csv('/home/aistudio/validate.tsv',sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\r\n",
    "testing.to_csv('/home/aistudio/testing.tsv',sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\r\n",
    "ming_data.to_csv('/home/aistudio/data/dataset.tsv',sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\r\n",
    "\r\n",
    "# predict_data is the Jin biographical data without tags\r\n",
    "predict_data=pd.read_table('/home/aistudio/test_data.txt')\r\n",
    "\r\n",
    "# 'content_without_name' consists of characters in the texts\r\n",
    "text=predict_data['content_without_name']\r\n",
    "\r\n",
    "# Also save it as .tsv files in case\r\n",
    "text.to_csv('/home/aistudio/predict.tsv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next block, we load the dataset into the **DemoDataset** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlehub.dataset.base_nlp_dataset import BaseNLPDataset\r\n",
    "\r\n",
    "# Construct the dataset for task1\r\n",
    "class DemoDataset(BaseNLPDataset):\r\n",
    "    \"\"\"DemoDataset\"\"\"\r\n",
    "    def __init__(self):\r\n",
    "        # Position where the data is stored\r\n",
    "        self.dataset_dir = \"/home/aistudio/\"\r\n",
    "        super(DemoDataset, self).__init__(\r\n",
    "            base_path=self.dataset_dir,\r\n",
    "            # train, validate and test\r\n",
    "            train_file=\"train.tsv\",\r\n",
    "            dev_file=\"validate.tsv\",\r\n",
    "            test_file=\"testing.tsv\",\r\n",
    "            # If we have any predict dataset\r\n",
    "            predict_file=\"predict.tsv\",\r\n",
    "            train_file_with_header=True,\r\n",
    "            dev_file_with_header=True,\r\n",
    "            test_file_with_header=True,\r\n",
    "            predict_file_with_header=True,\r\n",
    "            # the label maps. This list contain all tags in our training data\r\n",
    "            label_list=[\"O\",\r\n",
    "        \"B-date-reign\", \"I-date-reign\",\r\n",
    "        \"B-date-year\", \"I-date-year\",\r\n",
    "        \"B-office-voa\", \"I-office-voa\",\r\n",
    "        \"B-office-title\", \"I-office-title\",\r\n",
    "        \"B-place-placename\", \"I-place-placename\"])\r\n",
    "\r\n",
    "# load data\r\n",
    "task1_dataset = DemoDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next blocks, we define the training task. To define the training task, we shall first set up various configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construct Reader to preprocess the data\r\n",
    "reader = hub.reader.SequenceLabelReader(\r\n",
    "        #specify the dataset (the task1_dataset we've just defined above)\r\n",
    "        dataset=task1_dataset,\r\n",
    "        vocab_path=model.get_vocab_path(),\r\n",
    "        # Maximum sequence length, must be longer than any of the sequence\r\n",
    "        # in train, validate, test and predict\r\n",
    "        max_seq_len=512)\r\n",
    "\r\n",
    "#Select the optimizaion strategy\r\n",
    "strategy=hub.AdamWeightDecayStrategy(\r\n",
    "    weight_decay=0.01,\r\n",
    "    learning_rate=1e-4,\r\n",
    "    warmup_proportion=0.1\r\n",
    ")\r\n",
    "\r\n",
    "# Define the running configurations. To be more specific, hyperparameters\r\n",
    "config=hub.RunConfig(\r\n",
    "    # When using GPUs, please turn this on\r\n",
    "    use_cuda=True,\r\n",
    "    # Number of Epochs\r\n",
    "    num_epoch=80,\r\n",
    "    # Specify a filename containing the checkpoint records\r\n",
    "    # The training log will be saved in the filename specified\r\n",
    "    checkpoint_dir=\"chinese_wwm_base_seq_label_demo\",\r\n",
    "    # Specify the batch size\r\n",
    "    batch_size=16,\r\n",
    "    # For how many intervals the model evaluate the F1, loss, recall, etc.\r\n",
    "    # The evaluatio results are saved in checkpoint_dir\r\n",
    "    eval_interval=50,\r\n",
    "    strategy=strategy\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construct fine tune task(for task 1)\r\n",
    "inputs, outputs, program = model.context(\r\n",
    "    trainable=True, max_seq_len=512)\r\n",
    "\r\n",
    "sequence_output=outputs[\"sequence_output\"]\r\n",
    "\r\n",
    "feed_list=[\r\n",
    "    inputs[\"input_ids\"].name,\r\n",
    "    inputs[\"position_ids\"].name,\r\n",
    "    inputs[\"segment_ids\"].name,\r\n",
    "    inputs[\"input_mask\"].name\r\n",
    "]\r\n",
    "\r\n",
    "#Please again make sure that Paddlehub is of 1.8.2 version, or there will be error（parameter has no attribute...)\r\n",
    "seq_label_task1 = hub.SequenceLabelTask(\r\n",
    "    data_reader=reader,\r\n",
    "    feature=sequence_output,\r\n",
    "    feed_list=feed_list,\r\n",
    "    max_seq_len=512,\r\n",
    "    num_classes=task1_dataset.num_labels,\r\n",
    "    config=config,\r\n",
    "    # Add a layer of conditional random field\r\n",
    "    add_crf=True)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we start the fine-tune task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "# Execute finetune task\r\n",
    "task1_rate=seq_label_task1.finetune_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Paddlehub provides APIs to directly predict the data. However, data passed to the predict API should be in the list format, with each of its element another list containing the biography texts. To be specific, the data to be predict should be in the format like this:\n",
    "\n",
    "```py\n",
    "to_predict=[['曾任宣差總管鷹房打捕東勝等處渡河船隻河道所。'],\n",
    "['河內人，天會十四年參與修北村湯王廟。'],\n",
    "['孟津人，進義校尉，大定四年任河南路同知兼知陜州。']]\n",
    "```\n",
    "\n",
    "So if the data to predict is kept as a DataFrame object in the program, you shall use the following code to transform it into the format above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "#Execute predict task\r\n",
    "#import test_data to predict\r\n",
    "\r\n",
    "predict=[]\r\n",
    "for i in range(0,len(predict_data['content_without_name'])):\r\n",
    "    predict.append([predict_data['content_without_name'][i]])\r\n",
    "\r\n",
    "# Just to make sure it is really done :)\r\n",
    "print('done')\r\n",
    "\r\n",
    "#Rubbish code, ignore it\r\n",
    "#pred=seq_label_task_1.predict(data=predict_data['content_without_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And here comes the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict\r\n",
    "pred=seq_label_task1.predict(data=predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The next thing to do is to label the texts with the predict result. However, if you call the pred object, you will find that it is a list consists of addresses in the memory. For each address, if you call 'pred[i].run_results', you still get an integer instead of a specific tag. Actually, the integers involved corresponds to the tags (Like B-Place-Name, O, I-Reign, etc). So we have to convert these integers into tags to make them readable. This is not a very easy task.\n",
    "\n",
    "We first acquire the results in integer form and acquire the **results** object.\n",
    "\n",
    "If we call the results object, we will find that it is in a strange format. For each element in results, it consists of two arrays. The first array is a list of integers varying from 0-10 and the second array is a lists of positive integers. Let me explain why this will happen.\n",
    "\n",
    "The predict results are returned in batches. Each element in the results list is a batch of predictions. The first array in a elements are tagged results for all characters in this batch encoded in integers, while the second array encodes the number of biographies in each batch (the length of the 2nd array) and the length of each biography (the integers in the 2nd array). Hence, to reconstruct the results, we split the first array in each batch according to the lengths encoded in the 2nd array and use the inv_label_map to transform them into tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The output of paddlehub is written in a strange order\r\n",
    "# The following codes convert the output labels into readable tags\r\n",
    "\r\n",
    "# Acquire results that are shown as integers\r\n",
    "results=[p.run_results for p in pred]\r\n",
    "\r\n",
    "# The inverse label map that identifies which tag each integer shall correspond to\r\n",
    "# The reason why we know the mapping rule is that it follows the order of the tags in label_map\r\n",
    "# dictionary mentioned above\r\n",
    "inv_label_map={0:'O',1:'B-date-reign',2:'I-date-reign',3:'B-date-year',4:'I-date-year',\r\n",
    "5:'B-office-voa',6:'I-office-voa',7:'B-office-title',8:'I-office-title',9:'B-place-placename',\r\n",
    "10:'I-place-placename'}\r\n",
    "\r\n",
    "# Initialize the tags\r\n",
    "tags=[]\r\n",
    "\r\n",
    "# Transform integers to tags\r\n",
    "for num_batch, batch_results in enumerate(results):\r\n",
    "    infers = batch_results[0].reshape([-1]).astype(np.int32).tolist()\r\n",
    "    \r\n",
    "    #acquire the length of each text in batch #num_batch\r\n",
    "    np_lens = batch_results[1]\r\n",
    "    \r\n",
    "    \r\n",
    "    vernier=0\r\n",
    "\r\n",
    "    for index, np_len in enumerate(np_lens):\r\n",
    "        \r\n",
    "        #labels = infers[index * 400:(index + 1) * 400]\r\n",
    "        labels=infers[vernier:vernier+np_len]\r\n",
    "        vernier=vernier+np_len\r\n",
    "\r\n",
    "        label_str = []\r\n",
    "        count = 0\r\n",
    "        for label_val in labels:\r\n",
    "            label_str.append(inv_label_map[label_val])\r\n",
    "            count += 1\r\n",
    "            if count == np_len-1:\r\n",
    "                break\r\n",
    "        tags.append(label_str)\r\n",
    "\r\n",
    "\r\n",
    "# the first position of each prediction is a marker, throw away\r\n",
    "for i in range(0,len(tags)):\r\n",
    "    #print(i)\r\n",
    "    tags[i]=tags[i][1:len(tags[i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To make it looks more COOOL, we save the output as .txt files along with the biographies. Now you can read the biographies with tagged results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text=[]\r\n",
    "\r\n",
    "#Write predicted tags into .txt files\r\n",
    "\r\n",
    "for i in range(0,len(predict)):\r\n",
    "    text.append(predict[i][0])\r\n",
    "\r\n",
    "data={'text':text,'tags_predicted':tags}\r\n",
    "\r\n",
    "frame=pd.DataFrame(data)\r\n",
    "\r\n",
    "for i in range(0,len(text)):\r\n",
    "    print(i)\r\n",
    "    ner_result={'char':list(text[i]),'tag':list(tags[i])}\r\n",
    "    frame=pd.DataFrame(ner_result,columns=['char','tag'])\r\n",
    "    frame.to_csv('/home/aistudio/jin_ner_0203/'+str(bio_ids[i])+'.txt',sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.4 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
